{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project-1(SAI SRIKANTH SARABU).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vQTymL-rfML"
      },
      "source": [
        "##SAI SRIKANTH SARABU\n",
        "##A20343781"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "teDv9HGbrcsM",
        "outputId": "2654835a-0198-4851-b529-31636dbb42ee"
      },
      "source": [
        "!pip install pyspark"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.1.2.tar.gz (212.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 212.4 MB 68 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9\n",
            "  Downloading py4j-0.10.9-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 60.2 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.1.2-py2.py3-none-any.whl size=212880768 sha256=b6e86d4bbaf22263808ef2a7b2cb29bada08356d4c391cc9d4f71363a711a1b4\n",
            "  Stored in directory: /root/.cache/pip/wheels/a5/0a/c1/9561f6fecb759579a7d863dcd846daaa95f598744e71b02c77\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9 pyspark-3.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tE2MePzf2eYW",
        "outputId": "8c5a86d0-9181-48eb-bfbe-c160502a9a08"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agXDx90W2gPq"
      },
      "source": [
        "import sys\n",
        "from pyspark import SparkContext, SparkConf"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMH2ZAOC2iUZ"
      },
      "source": [
        "sc = SparkContext(\"local\",\"Project-1: A Simple Movie Recommender with Distributed Association Rules Analysis\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2dlRuHY2kbe"
      },
      "source": [
        "#All file Paths\n",
        "file1 = '/content/drive/MyDrive/Sample Data/views.txt'\n",
        "file2 = '/content/drive/MyDrive/Sample Data/mappings.txt'\n",
        "file3 = '/content/drive/MyDrive/Sample Data/get_recommendations.txt'"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dTxSj-y02n7C",
        "outputId": "9f729358-f6a6-45e0-e14b-ee062cdef39d"
      },
      "source": [
        "file =  sc.textFile(file1) \n",
        "completedata = file.map(lambda line:frozenset(line.split('::')))  #getting each line from the file and making it to frozenset\n",
        "file.unpersist()\n",
        "completedata.persist()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PythonRDD[2] at RDD at PythonRDD.scala:53"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56o2yXBYOr8X"
      },
      "source": [
        "#Generating frequent items\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JhroVqrG2q3x",
        "outputId": "4ce0d2e3-8bf9-4677-e9e0-af03e64520c5"
      },
      "source": [
        "s=3  #Initializing the support value\n",
        "\n",
        "#below line will generate frequent singletons \n",
        "singletons = file.flatMap(lambda line:line.split('::')).map(lambda x: frozenset((x,))).map(lambda key: (key , 1)).reduceByKey(lambda a,b: a+b).filter(lambda a: a[1]>=s)\n",
        "\n",
        "singletons.persist() #persisting the singleton data\n",
        "#singletons.take(5)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PythonRDD[7] at RDD at PythonRDD.scala:53"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hn4c08c4tSmJ"
      },
      "source": [
        "#the below function is used to generate n-sized combinations of given list x, returns list of lists of combinations\n",
        "def getCombs(x,n):\n",
        "  mylist =[]\n",
        "  if n==0:\n",
        "    return [mylist]\n",
        "  xlen = len(x)\n",
        "  for j in range(xlen): \n",
        "    klist = getCombs(x[j + 1:xlen], n-1) \n",
        "    #print(klist)\n",
        "    for i in klist:\n",
        "      mylist.append(list([x[j]])+i)\n",
        "  return mylist\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-UgJN-E6P_2"
      },
      "source": [
        "#below function is used for checking each item in previous frequent RDD with each line in complete data, \n",
        "#and making combinations with those items which are present.\n",
        "def combs(x,freqdata,k):  #here x is a line from complete data\n",
        "  mylist= []\n",
        "  a= frozenset()\n",
        "  for i in freqdata.value:   # looping through each item previous frequent data\n",
        "    if set(i).issubset(x):   # checking whether an item is present in line of complete data or not \n",
        "      a = a.union(i)         # preparing a list which are present\n",
        "  mylist = getCombs(list(a),k) # and making freq pairs, triplets, and quadruplets from the list of frequently present items.\n",
        "  \n",
        "  return mylist "
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uEzpuM0923LX"
      },
      "source": [
        "\n",
        "def frequent_itemsets(prev,k,freqcands):\n",
        "  prev = prev.keys()                              \n",
        "  prevbroad = sc.broadcast(prev.collect())      #broadcasting the previous frequent candidate items\n",
        "  combinedData=completedata.filter(lambda x: len(x)>=k)  #checking if each line length is greater than or equal to k for making combinations with k\n",
        "  combinedData = combinedData.flatMap(lambda x: combs(x,prevbroad,k))  # calling function combs with each line from complete data\n",
        "  #making the freq pairs, triplets or quadruplets to key and value with support \n",
        "  combinedData = combinedData.map(lambda x:(frozenset(x),1)).reduceByKey(lambda a,b: a+b).filter(lambda a: a[1]>=s)\n",
        "  freqcands = freqcands.union(combinedData)  #combining all the previous data with the latest\n",
        "  if k<=3:\n",
        "    return frequent_itemsets(combinedData,k+1,freqcands)  # calling the same function again by increasing k value.\n",
        "  return freqcands   "
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OUWfnQMZ5gkV",
        "outputId": "b848ea64-d9ae-4ad7-e795-b6862433a608"
      },
      "source": [
        "freqcands= sc.parallelize([])\n",
        "freqcands=frequent_itemsets(singletons,2,freqcands)  # calling the function frequent_itemsets\n",
        "#freqcands.count()\n",
        "freqcands = singletons.union(freqcands) #combining all singletons, pairs, triplets and quadruplets data\n",
        "singletons.unpersist()  #unpersisting the data as it is not required\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PythonRDD[7] at RDD at PythonRDD.scala:53"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kH-rlaANAwqj"
      },
      "source": [
        "\n",
        "dic = {}\n",
        "for d in freqcands.map(lambda x: dict([x])).collect(): # here we are making dictionary of freq items and support value\n",
        "  dic.update(d)\n",
        "\n",
        "freq_dic = sc.broadcast(dic) #broadcasting the dictionary"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRFJXKV6O4Ot"
      },
      "source": [
        "#Generating association rules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVjyeR6hhnwU"
      },
      "source": [
        "con_thres= 60 #initializing the confidence threshold value\n",
        "\n",
        "#below function is used to return antecedent, consequent and confidence of the association rules\n",
        "def assoc_rules(x):\n",
        "  x = x[0]\n",
        "  if len(x) == 1: #here we are avoiding sigletons as we need only higher order to generate association rules\n",
        "    return ''\n",
        "  else:\n",
        "    sup_num = freq_dic.value[x] #numerator of the confidence would be support value of all the elements together occuring\n",
        "    mylist = list(x)            #the above line is using freq_dic to get the support value\n",
        "    clist = []\n",
        "    for i in mylist:         # in this loop we are removing(this will be our consequent element) an each item once in iteration and making rules based on that. \n",
        "      tempList = mylist.copy() # making an copy of list\n",
        "      tempList.remove(i)       #removing an item in iteration\n",
        "      sup_den = freq_dic.value[frozenset(tempList)]  #getting support of antecedent elements\n",
        "      con = round((sup_num/sup_den)*100,2)  #finding confidence\n",
        "      if con >= con_thres:                  #checking whether it is present in threshold limit or not\n",
        "        clist.append((tempList,i,con))      #appending antecedent, consequent elements and confidence of association rule to a list\n",
        "\n",
        "  return clist"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JbosxPwdzWuJ",
        "outputId": "21728a9f-fbe5-4c80-95cd-6aadfbf0a5cd"
      },
      "source": [
        "association_rules = freqcands.flatMap(lambda x:assoc_rules(x)) # calling function assoc_rules(x), x being each frequent item.\n",
        "freqcands.unpersist()  #unpersisting the freqcands as we have no use further"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "UnionRDD[31] at union at NativeMethodAccessorImpl.java:0"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIoemfXmiULh"
      },
      "source": [
        "gmap =  sc.textFile(file2) #file 2 is mapping data for each id in complete data\n",
        "mapping = gmap.map(lambda line:line.split('::')) \n",
        "maping = {}\n",
        "for d in mapping.map(lambda x: dict([x])).collect(): # here we are making mapping into key value pairs using dictionary\n",
        "  maping.update(d)\n",
        "mappingsdict = sc.broadcast(maping) #broadcasting the mapping dictionary\n",
        "gmap.unpersist()  #unpersisting the data, which are not required\n",
        "mapping.unpersist()\n",
        "\n",
        "\n",
        "# making this dictionary to use in the bonus task\n",
        "bonusDict = {}\n",
        "#making a dictionary with key as antecedent and value as consequent and confidence\n",
        "for i, *rest in association_rules.map(lambda x: (frozenset(x[0]),(x[1],x[2]))).collect(): \n",
        "  bonusDict.setdefault(i, []).extend(rest)\n",
        "\n",
        "\n",
        "#the function below will map value of id with its movie name and return\n",
        "#here we are returning coma separated antecedent, consequent and confidence \n",
        "def get_res(x):\n",
        "  cList = []\n",
        "  b = mappingsdict.value[x[1]]\n",
        "  a = ''\n",
        "  for j in x[0]:\n",
        "    a+=', '+mappingsdict.value[j]\n",
        "  cList.append((a,b,x[2]))\n",
        "  return cList\n",
        "\n",
        "#the below line is formating the output and making confidence as key\n",
        "association_rules = association_rules.flatMap(lambda x:get_res(x)).map(lambda x : (x[2],x[0].strip(',') + '  ->  '+  x[1] +' ; Confidence = '+ str(x[2]) + ' % '))\n",
        "association_rules = association_rules.sortByKey(False).map(lambda x:x[1])#here we are sorting the elements in decreasing order of confidence\n",
        "association_rules.coalesce(1).saveAsTextFile('output.txt') #using coalesce to generate single file as output of association rules"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bq-SG9SoPFXo"
      },
      "source": [
        "#Bonus Task "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STWF9GN-iZPB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "461f865f-f87c-437d-81fe-2bf9fc4ecc85"
      },
      "source": [
        "count = 1 # initializing line count\n",
        "\n",
        "def bon(x):#here x is the list of single watch history\n",
        "  myList = []\n",
        "\n",
        "  if len(x)>=3:\n",
        "    for j in getCombs(x,3):#below 3 loops is generating combinations of watch history of size 1,2 and 3 as we have only antecedent size upto 3 in association rules \n",
        "      myList.append(j)\n",
        "  if len(x)>=2:\n",
        "    for o in getCombs(x,2):\n",
        "      myList.append(o) \n",
        "  if len(x)>=1:\n",
        "    for l in getCombs(x,1):\n",
        "      myList.append(l)\n",
        "  \n",
        "  cList = []\n",
        "  for i in myList:                #here we are looping through antecedent values and checking for a hit, and getting the recommendation as consequent\n",
        "    for r in bonusDict.keys():    #looping through all the antecedent values\n",
        "      if len(i) <= len(r):        #checking if length of the an item is less than antecedent value\n",
        "        if set(i).issubset(r):    #checking if it subset or not\n",
        "          cList.append(bonusDict[r])  #appending the consequent and confidence value, as value contains both\n",
        "          \n",
        "  global count\n",
        "  co = count\n",
        "  count+=1\n",
        "  if len(cList) > 0:\n",
        "    flat_list = [i for slist in cList for i in slist] #making list of lits to list, as each consequent and confidence is a list\n",
        "    flat_list.sort(key = lambda x: x[1], reverse = True) #arraning tupples in list in decreasing order\n",
        "    result = []\n",
        "    if len(cList)>=3:     #here we are recommenting top 3 movies in highest confidence\n",
        "      for i in range(3):\n",
        "        result.append(mappingsdict.value[flat_list[i][0]]) #flat list will give us id, based on that mapping dictionary will change id to movie name\n",
        "    else:\n",
        "      for i in range(len(cList)): #if we get less than 3 recommendations \n",
        "        result.append(mappingsdict.value[flat_list[i][0]])\n",
        "    return ('line '+ str(co) ,result)\n",
        "  else: # if we do not have any recommendations from the association rules\n",
        "    return 'line '+ str(co), 'No recommendation found from the above Association Rules'\n",
        "\n",
        "\n",
        "bonus =  sc.textFile(file3) #file 3 contains 10 watch histories\n",
        "bonus.map(lambda line:bon(line.split('::'))).collect() # calling the method bon with single watch history and k as parameter to get rcommendations\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('line 1',\n",
              "  ['Airplane! (1980)',\n",
              "   'American Beauty (1999)',\n",
              "   'Star Trek V: The Final Frontier (1989)']),\n",
              " ('line 2', 'No recommendation found from the above Association Rules'),\n",
              " ('line 3', 'No recommendation found from the above Association Rules'),\n",
              " ('line 4', 'No recommendation found from the above Association Rules'),\n",
              " ('line 5', 'No recommendation found from the above Association Rules'),\n",
              " ('line 6',\n",
              "  ['Cinderella (1950)',\n",
              "   'Beauty and the Beast (1991)',\n",
              "   'South Park: Bigger, Longer and Uncut (1999)']),\n",
              " ('line 7', 'No recommendation found from the above Association Rules'),\n",
              " ('line 8',\n",
              "  ['Terminator 2: Judgment Day (1991)',\n",
              "   'Jurassic Park (1993)',\n",
              "   'Raiders of the Lost Ark (1981)']),\n",
              " ('line 9', 'No recommendation found from the above Association Rules'),\n",
              " ('line 10', ['Groundhog Day (1993)', 'Player, The (1992)'])]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhq5hNjQTQ1P"
      },
      "source": [
        ""
      ],
      "execution_count": 16,
      "outputs": []
    }
  ]
}